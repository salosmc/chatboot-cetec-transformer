# -*- coding: utf-8 -*-
"""codigoChatBotV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UOtwEwfwIASAGJK4pBkUG-hQ9UUgPHfA
"""

#Recuerden que este es un .py, van a tener que hacer copy paste a su PC

# -*- coding: utf-8 -*-
#
from fastapi import FastAPI, Body, Request, Response, HTTPException, status
from fastapi.encoders import jsonable_encoder
from transformers import pipeline
from fastapi.middleware.cors import CORSMiddleware

# Importamos configuracion de base de datos y schemas
from config.db import collection_name

app = FastAPI()
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get('/pregunta/{name}')
async def geeter(name,request: Request):
    from transformers import AutoTokenizer, MarianMTModel
    from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
    import tensorflow as tf

    #Translate the question to english
    src = "es"  # source language
    trg = "en"  # target language
    model_name = f"Helsinki-NLP/opus-mt-{src}-{trg}"
    model = MarianMTModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    sample_text = name
    batch = tokenizer([sample_text], return_tensors="pt")
    generated_ids = model.generate(**batch)
    translated = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # --------------------- Plan de accion
    # 1 Creo que deberia buscar un array de clasificaciones de la base de datos para setear "candidate_labels"
    # 2 Luego la pregunta me permite ver que tipo de clasificacion es? se setea en "data"
    # 3 
    # -------------------------

    #First question clasification
    from transformers import pipeline 
    classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
    candidate_labels = ['kinematics', 'dynamics', 'rigid body kinematics', 'rigid body dinamics', 'work and enegy']
    data = classifier(translated, candidate_labels)['labels'][0]
    print(" ES ACA "+data)

    #ToDo find the text in the mongo database (the Salomon App) using the label data from the step before this
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased-distilled-squad")
    model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-cased-distilled-squad")
    question = translated

    #buscamos en la base de datos por "title"
    text = collection_name.find_one({"title": data})["description"]

    #text = '''
    #    It is called rectilinear movement, one whose trajectory is a straight line.
    #    On the line we place an origin O, where an observer will be who will measure the position of the mobile x at instant t. The positions will be positive if the mobile is to the right of the origin and negative if it is to the left of the origin.
    #    The position x of the mobile can be related to the time t by means of a function x=f(t).
    #    Suppose now that at time t, the mobile is in position x, later, at time t', the mobile will be in position x'. We say that the mobile has moved Δx=x'-x in the time interval Δt=t'-t, measured from instant t to instant t'.
    #    Velocity at an instant is defined as the derivative of position with respect to time.
    #    '''
    
    inputs = tokenizer(question, text, return_tensors="tf")
    outputs = model(**inputs)
    answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
    answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])
    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
    salida = tokenizer.decode(predict_answer_tokens)

    #Translate again to gibe the answer in spanish
    src = "en"  # source language
    trg = "es"  # target language
    model_name = f"Helsinki-NLP/opus-mt-{src}-{trg}"
    model = MarianMTModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    sample_text = salida
    batch = tokenizer([sample_text], return_tensors="pt")
    generated_ids = model.generate(**batch)
    translated = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    sample_text = data
    batch = tokenizer([sample_text], return_tensors="pt")
    generated_ids = model.generate(**batch)
    data = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return [translated, data]